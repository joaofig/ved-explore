{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Clustering (DBSCAN)\n",
    "\n",
    "**WARNING**: This is an experimental notebook and is not discussed in the article \"Geographic Clustering with HDBSCAN\".\n",
    "\n",
    "In this notebook we calculate the areas where the trips most commonly start and end. We call these the _trip endpoints_. Usually you also find these named as \"_stay points_\" in the literature but, due to the nature of the data that we are handling, where vehicles do not stop at these locations, we will keep the name \"_endpoint_\".\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Please run the `02-import-data.ipynb` notebook first, in case you need to buid the supporting SQLite database.\n",
    "- Required install: [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/user_install.html). Enable using `jupyter nbextension enable --py widgetsnbextension --sys-prefix` for Jupyter Notebook and `jupyter labextension install @jupyter-widgets/jupyterlab-manager` for Jupyter Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sqlapi import VedDb\n",
    "from h3 import h3\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from folium.vector_layers import CircleMarker\n",
    "from colour import Color\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by instantiating the `VedDb` object to interface with the SQLite database created and prepared in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VedDb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we collect the start and end locations of all the moves in the database as latitude and longitude pairs. Note that we have to join the `move` table to the `signal` table twice in order to get both start and end loctions. The result is downloaded into a DataFrame for in-memory manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select     m.move_id\n",
    ",          si.latitude  as lat_ini\n",
    ",          si.longitude as lon_ini\n",
    ",          se.latitude  as lat_end\n",
    ",          se.longitude as lon_end\n",
    "from       move m\n",
    "inner join signal si on si.day_num = m.day_num and \n",
    "                        si.vehicle_id = m.vehicle_id and \n",
    "                        si.time_stamp = m.ts_ini\n",
    "inner join signal se on se.day_num = m.day_num and \n",
    "                        se.vehicle_id = m.vehicle_id and \n",
    "                        se.time_stamp = m.ts_end\n",
    "\"\"\"\n",
    "df_pt = db.query_df(sql)\n",
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for each move we collect the start location `(lat_ini, lon_ini)` and destination location `(lat_end, lon_end)`. Using this data, we can determine the implied geographical clusters using DBSCAN. Before running that algorithm, we must first collect all locations into a single array, keeping the original ordering. The first half of the array contains the start locations while the second part contains the end locations. This is important in order to guarantee that we can assign the detetcted cluster identifiers back to the `move` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_ini = df_pt[['lat_ini', 'lon_ini']].to_numpy()\n",
    "loc_end = df_pt[['lat_end', 'lon_end']].to_numpy()\n",
    "locations = np.vstack((loc_ini, loc_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the first phase of the clustering process, we must estimate the value for $\\epsilon$, the maximum distance between points. Here, we follow the methodology described here: [DBSCAN Python Example: The Optimal Value For Epsilon (EPS)](https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=2,\n",
    "                      metric='haversine',\n",
    "                      algorithm='ball_tree',\n",
    "                      n_jobs=-1)\n",
    "pts = np.radians(locations)\n",
    "nbrs = nn.fit(pts)\n",
    "distances, indices = nbrs.kneighbors(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the sorted distances and look for the approximate location of the curve's elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.sort(distances, axis=0)\n",
    "dist = dist * 40070000.0 # Earth radius in meters\n",
    "plt.plot(dist[:, 1])\n",
    "plt.ylim((0, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above chart, we can postulate $ \\epsilon $ = 50.\n",
    "\n",
    "Now that we have all the locations ready we can cluster them using DBSCAN. The following function (`cluster_locations`) does just that. It assumes that clusters are valid when you can collect at least ten vehicles with distances of at most 25 meters in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clusters(locations, eps_in_meters=50.0, num_samples=20):\n",
    "    pts = np.radians(locations)\n",
    "\n",
    "    # Cluster the data\n",
    "    earth_perimeter = 40070000.0  # In meters\n",
    "    eps_in_radians = eps_in_meters / earth_perimeter * (2 * math.pi)\n",
    "\n",
    "    clusters = DBSCAN(eps=eps_in_radians, \n",
    "                      min_samples=num_samples,\n",
    "                      metric='haversine',\n",
    "                      algorithm='ball_tree').fit_predict(pts)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the code against the location array and collect the cluster identifiers in the same order as that of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = calculate_clusters(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of unique cluster identifiers is calculated below by removing the first unique value (-1), the noise indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = np.unique(clusters)[1:]\n",
    "print(\"The initial number of clusters is: {0}\".format(unique_clusters.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Cluster Exploration\n",
    "\n",
    "Here you can explore all the generated clusters through the interactive widget below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_map(cluster_id):\n",
    "    map = folium.Map(prefer_canvas=True, control_scale=True)\n",
    "    cluster_points = locations[clusters == cluster_id]\n",
    "    \n",
    "    for point in cluster_points:\n",
    "        CircleMarker(point, radius=1).add_to(map)\n",
    "        \n",
    "    min_lat, max_lat = cluster_points[:, 0].min(), cluster_points[:, 0].max()\n",
    "    min_lon, max_lon = cluster_points[:, 1].min(), cluster_points[:, 1].max()\n",
    "    map.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = interact(show_cluster_map, cluster_id=widgets.IntSlider(min=0, max=clusters.max(), step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Refinement\n",
    "\n",
    "We can now refine the existing clusters by running the same clustering algorithm on each cluster, but with different parameters. Let us first illustrate how to go about this using the selected cluster. We start by selecting the cluster locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_points = locations[clusters==cluster_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `cluster_locations` to further refine the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cluster_ids = calculate_clusters(cluster_points, eps_in_meters=12.5, num_samples=10)\n",
    "sub_clusters = np.unique(sub_cluster_ids[sub_cluster_ids >= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {0} sub clusters.\".format(sub_clusters.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to plot the clusters on a map using a color gradient to tell them apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_points(clusters, locations, colors):\n",
    "    map = folium.Map(prefer_canvas=True, control_scale=True)\n",
    "    \n",
    "    for cluster in np.unique(clusters):\n",
    "        for point in locations[clusters == cluster]:\n",
    "            CircleMarker(point, radius=1, \n",
    "                         color=colors[cluster].hex if cluster >= 0 else \"gray\",\n",
    "                         tooltip=\"{0}\".format(cluster)).add_to(map)\n",
    "    \n",
    "    min_lat, max_lat = locations[:, 0].min(), locations[:, 0].max()\n",
    "    min_lon, max_lon = locations[:, 1].min(), locations[:, 1].max()\n",
    "    map.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=list(Color(\"red\").range_to(\"green\", len(sub_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_points(sub_cluster_ids, cluster_points, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a number of points that were classified as noise. We can try to recover them back to the clusters by assigning them using a KNN-based approach. The function below, `reassign_noise_points`, does just that by returning the indices array of the `NearestNeighbors` call. These indices are then used to reassign the noise points to the nearest cluster, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_noise_points(cluster_ids, cluster_pts, k=7):\n",
    "    nn = NearestNeighbors(n_neighbors=k,\n",
    "                      metric='haversine',\n",
    "                      algorithm='ball_tree',\n",
    "                      n_jobs=-1)\n",
    "    pts = np.radians(cluster_pts)\n",
    "    nbrs = nn.fit(pts)\n",
    "    distances, indices = nbrs.kneighbors(pts)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call the `reassign_noise_points` function to get the indices back so we can work the reassignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = reassign_noise_points(sub_cluster_ids, cluster_points, k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through the _k-1_ nearest neighbor indices and sequentially assign the cluster identifier to the points that are still classified aas noise. Note that not all the noise points may be reassigned. You can try to increase the value of _k_ to assign more noise points to the clusters, but this will take more time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 7):\n",
    "    sub_cluster_ids[indices[sub_cluster_ids == -1, 0]] = sub_cluster_ids[indices[sub_cluster_ids == -1, i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the map looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_points(sub_cluster_ids, cluster_points, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the Process\n",
    "\n",
    "After illustrating the process, we are now goingo to automate it throughout the clustered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster_id = unique_clusters.max()\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_points = locations[clusters==cluster_id]\n",
    "    \n",
    "    sub_cluster_ids = calculate_clusters(cluster_points, eps_in_meters=12.5, num_samples=10)\n",
    "    sub_clusters = np.unique(sub_cluster_ids[sub_cluster_ids >= 0])\n",
    "    \n",
    "    if sub_clusters.shape[0] > 1:\n",
    "        print(\"Cluster {0} has {1} sub clusters and {2} noise points\".format(cluster_id, \n",
    "                                                                             sub_clusters.shape[0], \n",
    "                                                                             sub_cluster_ids[sub_cluster_ids == -1].shape[0]))\n",
    "        indices = reassign_noise_points(sub_cluster_ids, cluster_points, k=7)\n",
    "        for i in range(1, 7):\n",
    "            sub_cluster_ids[indices[sub_cluster_ids == -1, 0]] = sub_cluster_ids[indices[sub_cluster_ids == -1, i]]\n",
    "        \n",
    "        sub_cluster_ids[sub_cluster_ids==0] = cluster_id\n",
    "        for c in range(1, np.unique(sub_cluster_ids).shape[0]):\n",
    "            max_cluster_id += 1\n",
    "            sub_cluster_ids[sub_cluster_ids==c] = max_cluster_id\n",
    "        clusters[clusters == cluster_id] = sub_cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The final number of clusters is: {0}\".format(np.unique(clusters).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = interact(show_cluster_map, cluster_id=widgets.IntSlider(min=0, max=clusters.max(), step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize to the Database\n",
    "\n",
    "For the sake of future convenience, we will now create and fill in a table whith all the cluster points. This table will make our life much easier in the future, when handling clusters, their locations and shapes.\n",
    "\n",
    "We start by making sure that the table exists and is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not db.table_exists(\"cluster_point\"):\n",
    "    sql = \"\"\"\n",
    "    CREATE TABLE cluster_point (\n",
    "        pt_id           INTEGER PRIMARY KEY ASC,\n",
    "        cluster_id      INT NOT NULL,\n",
    "        latitude        FLOAT NOT NULL,\n",
    "        longitude       FLOAT NOT NULL,\n",
    "        h3              TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    db.execute_sql(sql)\n",
    "    db.execute_sql(\"CREATE INDEX idx_cluster_point_cluster ON cluster_point (cluster_id)\")\n",
    "else:\n",
    "    db.execute_sql(\"DELETE FROM cluster_point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can insert the locations for each cluster, along with the [H3](https://eng.uber.com/h3/) hexagon codes at [resolution level 12](https://uber.github.io/h3/#/documentation/core-library/resolution-table). In the next notebook, we will display the clusters using the outline of the collated shapes of all cluster hexagons.\n",
    "\n",
    "To prepare the insert statement, we now collect all the input data into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_level = 12\n",
    "cluster_points = []\n",
    "for i in tqdm(range(clusters.shape[0])):\n",
    "    if clusters[i] >= 0:\n",
    "        pt = (int(clusters[i]), locations[i, 0], locations[i, 1], h3.geo_to_h3(locations[i, 0], locations[i, 1], h3_level))\n",
    "        cluster_points.append(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now insert the cluster points into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.insert_cluster_points(cluster_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame can also be updated with the cluster identifiers like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = clusters.shape[0] // 2\n",
    "df_pt['cluster_ini'] = clusters[:n]\n",
    "df_pt['cluster_end'] = clusters[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the DataFrame looks like after cluster identifier assignment. Note that the constant `-1` means that the point was not assigned to any cluster, and was considered as noise instead. We will exclude these from the future trajectory analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the `move` Table\n",
    "\n",
    "Now that we have the clusters identified, we can assign them back to the `move` table. Unfortunately, this table has no columns where to store the clusters identifiers, so we must first handle that. For convenience, we also create an extra index on the new columns for more convenient search on them.\n",
    "\n",
    "**Notes**: \n",
    "- The function `table_has_column` tests whether a given table has a named column. Here we only test against one column as it is enough.\n",
    "- Depending on the time you use this code, the database may already have been created with these columns and indexes. If that is the case, the code below does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not db.table_has_column('move', 'cluster_ini'):\n",
    "    db.execute_sql(\"alter table move add cluster_ini INT not null default -1\")\n",
    "    db.execute_sql(\"alter table move add cluster_end INT not null default -1\")\n",
    "    db.execute_sql(\"create index idx_move_clusters on move (cluster_ini, cluster_end)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to update the `move` table using the recently calculated cluster identifiers. To do so, we use the data stored in the DataFrame to feed an update query. Note that we retrieve the data as a list of tuples with the eaxct order for consumption in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clids = list(df_pt[['cluster_ini', 'cluster_end', 'move_id']].itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.update_move_clusters(clids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now associated each move with a pair of endpoint clusters, and this allows us to perform more powerful analyses to the data, such as determine how many trips occur between two general endpoints, how many different trajectories there are and what vehicles have used them and at what time, consuming how much fuel or energy.\n",
    "\n",
    "Now, we turn to the issue of generating a geo-fence for the clusters so they can be easily displayed on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
