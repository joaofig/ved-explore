{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d901bb-42ee-49b6-b663-1bf4e5926f35",
   "metadata": {},
   "source": [
    "# 11-HDBSCAN-Generated Cluster Metric\n",
    "\n",
    "In this notebook we determine a metric for HDBSCAN-generated clusters. This metric is a surrogate for the missing DBSCAN's Îµ parameter that determines the maximum reachability distance for points in the cluster. Deriving such a metric is useful when determining H3 hexagon sizes to seamlessly cover the cluster, the inflate size for a concave hull-generated shape, or the maximum radius for bubble shaping.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Please run the `05-clustering-hdbscan.ipynb` notebook first and its dependencies.\n",
    "- Recommended install: [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/user_install.html). Enable using `jupyter nbextension enable --py widgetsnbextension --sys-prefix` for Jupyter Notebook and `jupyter labextension install @jupyter-widgets/jupyterlab-manager` for Jupyter Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f6aff-e44b-4e71-ab1c-7fb80bfb46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "import ipywidgets as widgets\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pyproj\n",
    "import utm\n",
    "import h3.api.numpy_int as h3\n",
    "\n",
    "from folium.vector_layers import PolyLine, CircleMarker, Polygon\n",
    "from db.api import VedDb\n",
    "from geo.math import vec_haversine, square_haversine, num_haversine\n",
    "from geo.hulls import ConcaveHull\n",
    "from shapely.geometry import Polygon as PolygonShape, Point as PointShape\n",
    "from shapely.ops import unary_union, transform\n",
    "from fitter import Fitter, get_common_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7163d-e530-43e3-bd44-77b7dc23adad",
   "metadata": {},
   "source": [
    "Select the cluster identifier to analyze below. These identifiers match the ones in the `cluster_id` column of the `cluster` database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0a818-7dc0-46c0-8dad-a17b6e4828d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59ff8c-6e27-4f6b-af55-e1e7933a200d",
   "metadata": {},
   "source": [
    "Now, we instantiate the database object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff7c7d-8cee-427b-975b-bedfa0283337",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VedDb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2624335-32e4-45b5-833b-63d4145460ec",
   "metadata": {},
   "source": [
    "The `get_cluster_locations` function retrieves a list of latitude and longitude pairs for all points in the given cluster. The single parameter is the cluster identifier, as generated by the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada3b3b-2a55-40c8-8153-2b28dbadba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = db.get_cluster_locations(cluster_id)\n",
    "locations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e2a12-f828-454e-9384-29b77fb2caa2",
   "metadata": {},
   "source": [
    "Calling the `square_haversine` function on the latitude and longitude lists returns a symmetric square matrix containing the pairwise distances between all locations. The function calculates these distances using a vectorized version of the Haversine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f643d82-739a-4826-8e55-49d8679aa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = square_haversine(locations[:, 0], locations[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3062cb-6f29-46e4-926d-15c42ec3c520",
   "metadata": {},
   "source": [
    "The `non_zero` function is a helper to filter out the diagonal zeros of each column in the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfb8de-3655-4ffc-ac40-8bd3b5989a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_zero(arr):\n",
    "    return arr[np.where(arr != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acb102-ceba-4143-86f2-32b436003f9b",
   "metadata": {},
   "source": [
    "Now, we use it to extract the list of minimum values on the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804648b9-0956-449f-9ecd-b4ad22af829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_location_minimums(locations):\n",
    "    dist = square_haversine(locations[:, 0], locations[:, 1])\n",
    "    minimums = [non_zero(dist[:, i]).min() for i in range(dist.shape[1])]\n",
    "    return minimums   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab32821-f636-468c-ae99-2fa3e0bb8df8",
   "metadata": {},
   "source": [
    "One could argue that the HDBSCAN equivalent of DBSCAN's $\\epsilon$ is the maximum of said minimum distances. For this case that value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e0daf-11e3-41e5-8769-91afedc3fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(get_per_location_minimums(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f72bf0-e594-4c71-8879-7a9409b05fe8",
   "metadata": {},
   "source": [
    "As we will see below, and for the purposes of finding a good measure for the cluster buffers, this might actually be an overestimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a8ef0-a5a8-4ddf-aba0-6d323786db49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b278785a-1126-4432-85ca-ad22a78e6597",
   "metadata": {},
   "source": [
    "## Statistics-Based Approach\n",
    "\n",
    "We now turn our exploration to using the minimum distance statistics to derive a reasonable cluster metric. The first approach is to assume that the minimum distances have a Normal distribution which, as we will later see, is _not correct_.\n",
    "\n",
    "The function below calculates the cluster distance metric, _assuming_ a Normal distribution, as $\\mu+2\\sigma$, that would correspond to roughly 95% of distance distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87509420-a0b6-41b7-aa07-11d21732d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_matrix_sig(locations, sigma_factor = 2.0):\n",
    "    minimums = get_per_location_minimums(locations)\n",
    "    return np.average(minimums) + np.std(minimums) * sigma_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4914d-d38a-4cf7-a9f8-1073749ee4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_matrix_sig = get_cluster_matrix_sig(locations)\n",
    "cluster_matrix_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff5965-e67c-485b-9be7-b93503d67b6e",
   "metadata": {},
   "source": [
    "By researching into the typical minimum distance distribution (see Notebook 12), we find that, for this dataset, most cluster minimum distances follow either a Log-Normal or a Gamma distribution. We can now see how the same approach as the Normal behaves under a Log-Normal.\n",
    "\n",
    "The function below assumes that the distance distrubution follows a Log-Normal distribution, computing the metric as $m+2s$, where $m=e^{\\mu+\\frac{\\sigma^{2}}{2}}$ and $s=m\\sqrt{e^{\\sigma^{2}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cb7f7-e896-40a6-807e-37d605f354c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_matrix_logsig(locations, sigma_factor = 2.0):\n",
    "    minimums = get_per_location_minimums(locations)\n",
    "    logs = np.log(minimums)\n",
    "    miu = np.average(logs)\n",
    "    sig = np.std(logs)\n",
    "    m = np.exp(miu + sig * sig / 2)\n",
    "    s = np.sqrt((np.exp(sig * sig) - 1)) * m\n",
    "    return m + s * sigma_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987691b-5b41-4847-b239-99c62ad2dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metric = get_cluster_matrix_logsig(locations, 2)\n",
    "cluster_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e4da4-36f3-4ee4-ba82-6653445c5c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5c15b-24cf-41d3-83c3-6e154b50d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_metric(minimums, factor=2.0):\n",
    "    logs = np.log(minimums)\n",
    "    mu = np.average(logs)\n",
    "    sigma = np.std(logs)\n",
    "    m = math.exp(mu + sigma * sigma / 2)\n",
    "    s = math.sqrt((np.exp(sigma * sigma) - 1)) * m\n",
    "    return m + s * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4680a9-7d6f-4349-bba7-3e3cd89785a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_metric(get_per_location_minimums(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bbb0d-ff94-4c67-92c8-c336eba4ecd2",
   "metadata": {},
   "source": [
    "The cell below times the execution of the per-location minimum calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f1f0a-b554-4ddc-8d88-075752fe4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "get_cluster_metric(get_per_location_minimums(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbc950-fdaf-4081-ae29-99353613dbf6",
   "metadata": {},
   "source": [
    "Here is the map representation for the cluster's locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dd7a5-c2ce-4849-9b44-3df7b79667b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bounding_box(html_map, bb_list):\n",
    "    if isinstance(bb_list, list):\n",
    "        ll = np.array(bb_list)\n",
    "    else:\n",
    "        ll = bb_list\n",
    "        \n",
    "    min_lat, max_lat = ll[:, 0].min(), ll[:, 0].max()\n",
    "    min_lon, max_lon = ll[:, 1].min(), ll[:, 1].max()\n",
    "    html_map.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
    "    return html_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f38aa-14f9-43af-a9ec-6c6c34ae3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_locations(html_map, locations):\n",
    "    for l in locations:\n",
    "        c = CircleMarker(l.tolist(), radius=2, color=\"red\", fill=\"red\", opacity=0.5)\n",
    "        c.add_to(html_map)\n",
    "    return html_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc73a3c-396c-40a1-8b33-1fd99da08fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20)\n",
    "\n",
    "html_map = draw_locations(html_map, locations)\n",
    "    \n",
    "fit_bounding_box(html_map, locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e0015-4942-4f04-864a-6a9a5755edfc",
   "metadata": {},
   "source": [
    "## Network Theory Approach\n",
    "We can alternatively look at this problem through the perspective of network theory. We start by generating a fully-connected undirected graph where each node is a location, and we weight the edges using the calculated distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897c692-d130-4579-b3cb-ad152f409670",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "n = locations.shape[0]\n",
    "g.add_nodes_from(range(n))\n",
    "g.add_edges_from([(i, j, {'weight': dist[i, j]}) for i in range(n-1) for j in range(i + 1, n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75a921-b25c-46e0-9843-3fd453a8d7c1",
   "metadata": {},
   "source": [
    "We now calculate the minimum spanning tree of the graph, using Prim's algorithm. The resulting edge weights correspond to the list of minimum distances we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bca0d-1add-4413-8e71-abe69e35fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mst = nx.minimum_spanning_tree(g, algorithm=\"prim\", weight=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcbd1a-d53f-456c-ba28-8f2fee24961e",
   "metadata": {},
   "source": [
    "Here is how the minimum spanning tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22be88-43e7-4850-b837-892e7cc60815",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(mst, node_size=20, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd79c6d-ef17-463b-a236-b6f717a9c2d1",
   "metadata": {},
   "source": [
    "The code below retrieves the minimum spanning tree edge distances. Note how we must use the distance matrix calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd08d-d1c7-49c6-a778-096b55dbe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = [dist[e[0], e[1]] for e in mst.edges()]\n",
    "\n",
    "max(min_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0862b-d493-412d-8e89-ad4e37fd0fd7",
   "metadata": {},
   "source": [
    "Again, we package all of this code into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be0f55-0557-4329-bf9b-753e59498412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_mst_minimums(locations):\n",
    "    n = locations.shape[0]\n",
    "    g = nx.Graph()\n",
    "    \n",
    "    dist = square_haversine(locations[:, 0], locations[:, 1])\n",
    "\n",
    "    g.add_nodes_from(range(locations.shape[0]))\n",
    "    g.add_edges_from([(i, j, {'weight': dist[i, j]}) for i in range(n) for j in range(i + 1, n)])\n",
    "            \n",
    "    mst = nx.minimum_spanning_tree(g, algorithm='prim', weight=\"weight\")\n",
    "    min_dist = [dist[e[0], e[1]] for e in mst.edges() if dist[e[0], e[1]] > 0.0]\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e0982-deb0-4a3c-b76b-16ae7186c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metric = get_cluster_metric(get_graph_mst_minimums(locations))\n",
    "cluster_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf2ced-b34a-40ec-bd1f-2146357c0cdb",
   "metadata": {},
   "source": [
    "As you can see, the result is a bit larger than the per-location one. You can iterate through other clusters by re-running the notebook after changing the cluster identifier above. Unfortunately, this approach is a full order of magnitude slower that the distance matrix approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3460b5d-43b4-4d2f-a1ca-9d725d9b9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "get_cluster_metric(get_graph_mst_minimums(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfab43-047b-4007-8b1a-1794c09614a4",
   "metadata": {},
   "source": [
    "The function below, `map_with_mst`, displays the cluster along with an overlayed representation of the minimum spanning tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321deecd-9346-469e-95be-b2058c9814f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_with_mst(locations, mst):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "    for e in mst.edges():\n",
    "        line = [[locations[e[0], 0], locations[e[0], 1]], [locations[e[1], 0], locations[e[1], 1]]]\n",
    "        l = PolyLine(line, weight=2, opacity=0.5)\n",
    "        l.add_to(html_map)\n",
    "        \n",
    "    html_map = draw_locations(html_map, locations)\n",
    "\n",
    "    return fit_bounding_box(html_map, locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188bb04-6f45-4e1e-90f0-542c413b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_with_mst(locations, mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5770f6-dcfe-4d7a-996a-8c8338497942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ac184c-2cb3-4cd8-95b8-ab217ae65d35",
   "metadata": {},
   "source": [
    "## Minimum Distance Distribution\n",
    "\n",
    "Now we inspect how the minimum distance distribution looks like, and what distribution best fits. Here, we use the `fitter` package to discover the best-fitting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a14a6-3c21-43a5-a8cd-832242fa2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Fitter(min_dist, distributions=get_common_distributions())\n",
    "f.fit()\n",
    "f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784a867-639d-42b7-bd2c-33dcf6e8069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.get_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b824b0a-a607-44e7-9ade-2a83ec4dee37",
   "metadata": {},
   "source": [
    "## Log Normal Distribution Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aa1fd-17cc-40e8-ac33-bdf9c28e1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c, b, p = ax.hist(min_dist, bins=20)\n",
    "\n",
    "fit_alpha, fit_loc, fit_beta = stats.lognorm.fit(min_dist)\n",
    "\n",
    "x = np.linspace(0.0, max(b))\n",
    "y = stats.lognorm.pdf(x, fit_alpha, fit_loc, fit_beta)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, y, alpha=0.6, color=\"red\")\n",
    "\n",
    "ax.set(xlabel='Distance (m)', ylabel='Count',\n",
    "       title='Minimum Distances - Log Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ca11f-dd81-4db9-84a4-9d691011d699",
   "metadata": {},
   "source": [
    "### Gamma Distribution Fit\n",
    "Here we try to fit the minimum distances to the Gamma distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370cfcd-6551-470c-997c-a72164a2d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c, b, p = ax.hist(min_dist, bins=20)\n",
    "\n",
    "fit_alpha, fit_loc, fit_beta = stats.gamma.fit(min_dist)\n",
    "\n",
    "x = np.linspace(0.0, max(b))\n",
    "y = stats.gamma.pdf(x, fit_alpha, fit_loc, fit_beta)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, y, alpha=0.6, color=\"red\")\n",
    "\n",
    "ax.set(xlabel='Distance (m)', ylabel='Count',\n",
    "       title='Minimum Distances - Gamma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73279dcb-7fc9-49df-af76-a0b12bc73c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a1e7cf8-f355-478e-b2c3-0f03d80160dd",
   "metadata": {},
   "source": [
    "# Concave Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e3f3b-84cf-464f-b2c2-a5e0ff3ed361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concave_hull_shape(locations, k):\n",
    "    hull = ConcaveHull([[x[1], x[0]] for x in locations])\n",
    "    shape = hull.calculate(k)\n",
    "    shape_latlng = np.array([[x[1], x[0]] for x in shape])\n",
    "    return shape_latlng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6a896-ad1c-485b-bd14-cb97d6f9953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_concave_hull(html_map, locations, k):\n",
    "    shape_latlng = get_concave_hull_shape(locations, k)\n",
    "    \n",
    "    polygon = Polygon(shape_latlng, weight=1, opacity=0.5)\n",
    "    polygon.add_to(html_map)\n",
    "    return html_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a34b49-0f33-486f-b8d2-566ba7feb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hull(locations, k=3):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "    \n",
    "    html_map = draw_concave_hull(html_map, locations, k)\n",
    "    html_map = draw_locations(html_map, locations)\n",
    "\n",
    "    return fit_bounding_box(html_map, locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab044c3f-15d9-4a7e-bd9c-2e9b8b9a14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_hull(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5fd15-8d9c-4859-aa36-d8e2a9af55db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4060f-ce2f-4d1f-9716-f0b543fb5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_in_meters(shape, meters):\n",
    "    xs, ys, zn, zl = utm.from_latlon(shape[:,1], shape[:,0])\n",
    "\n",
    "    polygon = PolygonShape(np.array([xs, ys]).T)\n",
    "\n",
    "    buffer_meters = polygon.buffer(meters)\n",
    "\n",
    "    xs = np.array([l[0] for l in buffer_meters.exterior.coords])\n",
    "    ys = np.array([l[1] for l in buffer_meters.exterior.coords])\n",
    "    \n",
    "    lats, lngs = utm.to_latlon(xs, ys, zn, zl)\n",
    "\n",
    "    return lats, lngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488e748-7211-42aa-a092-9a06a6fe864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_buffered_hull(locations, cluster_metric, k=3, metric_factor=1.0):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "    \n",
    "    shape_latlng = get_concave_hull_shape(locations, k)\n",
    "    polygon = Polygon(shape_latlng, weight=1, opacity=0.5)\n",
    "    polygon.add_to(html_map)\n",
    "    \n",
    "    lats, lngs = buffer_in_meters(np.array([[x[1], x[0]] for x in shape_latlng]), \n",
    "                                  cluster_metric * metric_factor)\n",
    "    buffer_polygon = Polygon(np.array([lats, lngs]).T,\n",
    "                             weight=2, opacity=0.6, color=\"blue\")\n",
    "    buffer_polygon.add_to(html_map)\n",
    "\n",
    "    html_map = draw_locations(html_map, locations)\n",
    "    \n",
    "    print(lats.shape[0])\n",
    "    \n",
    "    return fit_bounding_box(html_map, shape_latlng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ccb1d-4932-4d36-89f0-6af88ccc8d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_buffered_hull(locations, cluster_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6239a-e31b-467b-9983-467743e1bd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5a2ecdc-6ce2-401e-87ff-77072c5424e9",
   "metadata": {},
   "source": [
    "# Bubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077d516-a7db-4115-a8eb-78992a37139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bubbles_shape(locations, metric_factor):\n",
    "    xs, ys, zn, zl = utm.from_latlon(locations[:,0], locations[:,1])\n",
    "\n",
    "    points = [PointShape([l[0], l[1]]) for l in zip(xs, ys)]\n",
    "    bubbles = [point.buffer(cluster_metric * metric_factor) for point in points]\n",
    "    final_shape = unary_union(bubbles)\n",
    "\n",
    "    xs = np.array([l[0] for l in final_shape.exterior.coords])\n",
    "    ys = np.array([l[1] for l in final_shape.exterior.coords])\n",
    "\n",
    "    lats, lngs = utm.to_latlon(xs, ys, zn, zl)\n",
    "    shape_latlng = np.array([lats, lngs]).T\n",
    "    return shape_latlng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59a3b1-2397-473a-834a-ff0550f588ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bubbles(html_map, shape_latlng, metric_factor):\n",
    "    polygon = Polygon(shape_latlng, weight=2, opacity=0.6)\n",
    "    polygon.add_to(html_map)\n",
    "    return html_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867e7cc-adf0-4d21-bae8-33ac050de14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bubbles(locations, cluster_metric, metric_factor=1.0):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "\n",
    "    shape_latlng = get_bubbles_shape(locations, metric_factor)\n",
    "\n",
    "    html_map = draw_bubbles(html_map, shape_latlng, metric_factor)\n",
    "    html_map = draw_locations(html_map, locations)\n",
    "    \n",
    "    print(shape_latlng.shape[0])\n",
    "    \n",
    "    return fit_bounding_box(html_map, shape_latlng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dad79-21aa-4f6b-ad2b-b186debbce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_bubbles(locations, cluster_metric, metric_factor=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f8adc-de82-4b23-9484-399fa405f423",
   "metadata": {},
   "source": [
    "# H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bedc8e-e48f-4812-b3ac-7e94cc4be328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a489c25-a6db-4d60-af9d-8bc4d52f7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_resolution(meters):\n",
    "    h3_side_km = np.array(\n",
    "        [1107.712591000,\n",
    "           418.676005500,\n",
    "           158.244655800,\n",
    "            59.810857940,\n",
    "            22.606379400,\n",
    "             8.544408276,\n",
    "             3.229482772,\n",
    "             1.220629759,\n",
    "             0.461354684,\n",
    "             0.174375668,\n",
    "             0.065907807,\n",
    "             0.024910561,\n",
    "             0.009415526,\n",
    "             0.003559893,\n",
    "             0.001348575,\n",
    "             0.000509713])\n",
    "    kms = meters / 1000.0\n",
    "    return np.argwhere(h3_side_km < kms)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802dc46-e927-48d3-95d3-77e87b8903ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_res = get_h3_resolution(cluster_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453de57-406e-4b5d-8c50-35b7ab8b5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metric, h3_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbda1e-4c83-4400-9d4a-9bd20203107d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafb971-3e52-4e1d-939e-812db0cfd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hexagon(h):\n",
    "    geo_lst = list(h3.h3_to_geo_boundary(h))\n",
    "    geo_lst.append(geo_lst[0])\n",
    "    return np.array(geo_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a755380-7382-4afe-953f-31241d781339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f750b2-c6ba-428d-890b-537cccb4640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_polygon(xy, tooltip='',\n",
    "                       color='#3388ff',\n",
    "                       opacity=0.7,\n",
    "                       # fill_color='#3388ff',\n",
    "                       # fill_opacity=0.4, \n",
    "                       weight=3):\n",
    "    points = [[x[0], x[1]] for x in xy]\n",
    "    polygon = folium.vector_layers.Polygon(locations=points,\n",
    "                                           tooltip=tooltip,\n",
    "                                           # fill=True,\n",
    "                                           color=color,\n",
    "                                           # fill_color=fill_color,\n",
    "                                           # fill_opacity=fill_opacity,\n",
    "                                           weight=weight,\n",
    "                                           opacity=opacity)\n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470d58f-cac2-4c93-b3cf-01163cffade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_hexagons(locations, h3_res):\n",
    "    bb_list = []  # List for the bounding-box calculation\n",
    "    polygons = []\n",
    "    hexes = list(set([h3.geo_to_h3(l[0], l[1], h3_res) for l in locations]))\n",
    "    \n",
    "    for h in hexes:\n",
    "        points = get_hexagon(h)\n",
    "        xy = [[x[1], x[0]] for x in points]\n",
    "        xy.append([points[0][1], points[0][0]])\n",
    "        polygons.append(PolygonShape(xy))\n",
    "        bb_list.extend(points)\n",
    "        \n",
    "    merged = unary_union(polygons)\n",
    "    return merged, bb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69db5a-7dc3-466b-ae84-03ce4b0c0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_cluster(locations, cluster_metric, metric_factor=1.0):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "    h3_res = get_h3_resolution(cluster_metric * metric_factor)\n",
    "\n",
    "    merged, bb_list = get_merged_hexagons(locations, h3_res)\n",
    "    \n",
    "    if merged.geom_type == \"MultiPolygon\":\n",
    "        max_len = 0\n",
    "        largest = None\n",
    "        for geom in merged.geoms:\n",
    "            xy = geom.exterior.coords.xy\n",
    "            lxy = list(zip(xy[1], xy[0]))\n",
    "            create_map_polygon(lxy).add_to(html_map)\n",
    "    elif merged.geom_type == \"Polygon\":\n",
    "        xy = merged.exterior.coords.xy\n",
    "        lxy = list(zip(xy[1], xy[0]))\n",
    "        create_map_polygon(lxy).add_to(html_map)\n",
    "\n",
    "    html_map = draw_locations(html_map, locations)\n",
    "\n",
    "    return fit_bounding_box(html_map, bb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e3f60-1c7a-40aa-8e26-6fdac1d8f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map_cluster(locations, cluster_metric, metric_factor=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cf345-937b-4c55-ac50-75990ffaab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a34046-61ff-4dc1-941f-cf8fc2b583f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb5a5b-9214-43b0-9699-bd2c2391aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_map(locations):\n",
    "    html_map = folium.Map(prefer_canvas=True, tiles=\"cartodbpositron\", max_zoom=20, control_scale=True)\n",
    "    \n",
    "    html_map = draw_locations(html_map, locations)\n",
    "    return html_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
